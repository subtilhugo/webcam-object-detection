<<<<<<< HEAD
# Demo SmolVLM en temps rÃ©el avec votre webcam

Cette application web vous permet d'interroger un modÃ¨le **SmolVLM** via un serveur
`llama.cpp` pour identifier des objets dans le flux de votre webcam. Toutes les
infÃ©rences se font localement : aucune image n'est envoyÃ©e vers un service
externe. Vous devez disposer d'un ordinateur compatible avec le modÃ¨le
multimodal et d'une connexion internet uniquement pour tÃ©lÃ©charger les
fichiers nÃ©cessaires.

## PrÃ©â€‘requis

1. **Installer `llama.cpp`** :
   ```bash
   # Cloner le dÃ©pÃ´t officiel
   git clone https://github.com/ggml-org/llama.cpp
   cd llama.cpp
   # Compiler les binaires
   make -j$(nproc)
   make server
   ```

2. **DÃ©marrer le serveur multimodal** :
   L'outil `llama-server` peut tÃ©lÃ©charger automatiquement le modÃ¨le depuis
   Hugging Face. ExÃ©cutez cette commande dans le dossier `llama.cpp` :
   ```bash
   ./server/llama-server -hf ggml-org/SmolVLM-500M-Instruct-GGUF
   ```
   - Par dÃ©faut le serveur Ã©coute sur `http://localhost:8080`. 
   - Si vous disposez d'un GPU compatible, ajoutez l'option `-ngl 99` pour
     activer l'accÃ©lÃ©ration (par exemple : `./server/llama-server -hf â€¦ -ngl 99`).

3. **HÃ©berger l'interface web** :
   Dans le dossier contenant ce fichier (`smolvlm_realtime_webcam`), lancez
   un petit serveur HTTP local :
   ```bash
   cd smolvlm_realtime_webcam
   python3 -m http.server 5500
   ```
   Cela dÃ©marre un serveur sur `http://localhost:5500` qui sert la page
   `index.html`. L'utilisation d'un serveur local est nÃ©cessaire pour que le
   navigateur accepte l'accÃ¨s Ã  la webcam.

4. **Ouvrir l'interface** :
   Dans votre navigateur (Chrome ou Edge de prÃ©fÃ©rence), allez sur
   `http://localhost:5500/index.html`.
   - Lorsque la page charge, le navigateur vous demandera l'autorisation
     d'accÃ©der Ã  la camÃ©ra. Acceptez cette demande.
   - Le champ Â« Base API Â» doit rester `http://localhost:8080` si vous
     utilisez le serveur `llama.cpp` local.
   - Le champ Â« Instruction Â» contient par dÃ©faut la question *Â« What do you
     see? Â»*. Vous pouvez la modifier, par exemple : Â« Quels sont les objets
     dans cette scÃ¨ne ? Â» ou demander une rÃ©ponse JSON.
   - Cliquez sur **Start** pour dÃ©marrer l'envoi d'images au serveur. Le
     modÃ¨le rÃ©pondra dans le champ Â« Response Â».

## Remarques

- L'application envoie une image de la camÃ©ra et l'instruction au serveur Ã 
  intervalles rÃ©guliers (par dÃ©faut toutes les 500 ms). Ajustez ce dÃ©lai Ã 
  l'aide du menu dÃ©roulant Â« Interval Â» pour trouver le bon compromis entre
  rÃ©activitÃ© et consommation de ressources.
- Le modÃ¨le **SmolVLM 500M** est relativement lÃ©ger (~437 Mo en quantification
  8 bits) mais nÃ©cessite tout de mÃªme quelques gigaoctets de RAM pour
  l'infÃ©rence. Assurezâ€‘vous que votre machine dispose de suffisamment de
  mÃ©moire.
- Pour des rÃ©ponses structurÃ©es (par exemple en JSON), modifiez
  l'instruction en consÃ©quence, par exemple : Â« DÃ©cris les objets prÃ©sents et
  renvoie uniquement une liste JSON contenant leurs noms. Â»

En suivant ces Ã©tapes, vous obtiendrez une interface web locale qui utilise
votre webcam et `llama.cpp` pour identifier les objets en temps rÃ©el.
=======
# ðŸ” DÃ©tection d'Objets en Temps RÃ©el par Webcam

Une interface web moderne pour identifier des objets en temps rÃ©el en utilisant votre webcam avec l'intelligence artificielle. Supporte Llama.cpp (local, gratuit) et OpenAI (cloud, payant).

## âœ¨ FonctionnalitÃ©s

- ðŸŽ¥ **DÃ©tection en temps rÃ©el** via webcam
- ðŸ¤– **Support multi-modÃ¨les** : Llama.cpp et OpenAI
- ðŸŽ¨ **Interface moderne** et responsive
- âš¡ **Performance optimisÃ©e** avec gestion des intervalles
- ðŸ”§ **Configuration flexible** pour diffÃ©rents modÃ¨les
- ðŸ“± **Design responsive** pour mobile et desktop

## ðŸš€ Installation Rapide

### Option 1 : Llama.cpp (RecommandÃ© - Gratuit)

1. **Installer Llama.cpp** :
   ```bash
   chmod +x install_llama.sh
   ./install_llama.sh
   ```

2. **DÃ©marrer le serveur Llama.cpp** :
   ```bash
   chmod +x start_llama_server.sh
   ./start_llama_server.sh
   ```

3. **Lancer l'interface web** :
   ```bash
   python3 server.py
   ```

### Option 2 : OpenAI (Payant)

1. **Obtenir une clÃ© API OpenAI** :
   - Allez sur [OpenAI Platform](https://platform.openai.com/api-keys)
   - CrÃ©ez une nouvelle clÃ© API

2. **Lancer l'interface web** :
   ```bash
   python3 server.py
   ```

3. **Configurer dans l'interface** :
   - SÃ©lectionnez "OpenAI"
   - Entrez votre clÃ© API
   - Choisissez un modÃ¨le (GPT-4o recommandÃ©)

## ðŸ“‹ PrÃ©requis

### Pour Llama.cpp :
- macOS (testÃ© sur macOS 14+)
- Homebrew
- Au moins 8GB de RAM
- GPU recommandÃ© (Metal pour macOS)

### Pour OpenAI :
- Connexion internet
- ClÃ© API OpenAI valide
- Compte OpenAI avec crÃ©dits

## ðŸŽ¯ Utilisation

1. **Ouvrez l'interface** : http://localhost:8000
2. **Choisissez votre modÃ¨le** : Llama.cpp ou OpenAI
3. **Configurez les paramÃ¨tres** :
   - URL du serveur (pour Llama.cpp)
   - ClÃ© API (pour OpenAI)
   - Instructions personnalisÃ©es
   - Intervalle de capture
4. **Cliquez sur "DÃ©marrer"** et accordez les permissions de camÃ©ra
5. **Profitez** de la dÃ©tection d'objets en temps rÃ©el !

## âš™ï¸ Configuration AvancÃ©e

### Llama.cpp

**ModÃ¨les disponibles** :
- `SmolVLM-500M-Instruct-GGUF` (recommandÃ©, rapide)
- `llava-v1.5-7b` (plus prÃ©cis, plus lent)
- `bakllava-1` (bon Ã©quilibre)

**Options de serveur** :
```bash
# Utiliser le GPU
./llama-server -hf ggml-org/SmolVLM-500M-Instruct-GGUF -ngl 99

# Changer le port
./llama-server -hf ggml-org/SmolVLM-500M-Instruct-GGUF -p 8080

# Plus de mÃ©moire
./llama-server -hf ggml-org/SmolVLM-500M-Instruct-GGUF -c 4096
```

### OpenAI

**ModÃ¨les disponibles** :
- `gpt-4o` (recommandÃ©, trÃ¨s prÃ©cis)
- `gpt-4o-mini` (rapide, Ã©conomique)
- `gpt-4-vision-preview` (spÃ©cialisÃ© vision)

## ðŸ”§ DÃ©pannage

### ProblÃ¨mes de camÃ©ra
- **Erreur de permissions** : Assurez-vous d'accorder l'accÃ¨s Ã  la camÃ©ra
- **CamÃ©ra non dÃ©tectÃ©e** : VÃ©rifiez que votre webcam fonctionne
- **HTTPS requis** : Utilisez localhost ou HTTPS

### ProblÃ¨mes Llama.cpp
- **Serveur non accessible** : VÃ©rifiez que llama-server est en cours d'exÃ©cution
- **Erreur de compilation** : VÃ©rifiez les dÃ©pendances (cmake, ninja)
- **MÃ©moire insuffisante** : RÃ©duisez la taille du contexte (-c 2048)

### ProblÃ¨mes OpenAI
- **ClÃ© API invalide** : VÃ©rifiez votre clÃ© sur OpenAI Platform
- **Quota dÃ©passÃ©** : VÃ©rifiez vos crÃ©dits OpenAI
- **Erreur de rÃ©seau** : VÃ©rifiez votre connexion internet

## ðŸ“ Structure du Projet

```
webcam-object-detection/
â”œâ”€â”€ index.html          # Interface web principale
â”œâ”€â”€ server.py           # Serveur web Python
â”œâ”€â”€ install_llama.sh    # Script d'installation Llama.cpp
â”œâ”€â”€ start_llama_server.sh # Script de dÃ©marrage serveur
â”œâ”€â”€ README.md           # Ce fichier
â””â”€â”€ llama.cpp/          # Dossier Llama.cpp (aprÃ¨s installation)
```

## ðŸ¤ Contribution

Les contributions sont les bienvenues ! N'hÃ©sitez pas Ã  :
- Signaler des bugs
- Proposer des amÃ©liorations
- Ajouter de nouveaux modÃ¨les
- AmÃ©liorer l'interface

## ðŸ“„ Licence

Ce projet est sous licence MIT. Voir le fichier LICENSE pour plus de dÃ©tails.

## ðŸ™ Remerciements

- [llama.cpp](https://github.com/ggml-org/llama.cpp) - Pour l'infrastructure locale
- [SmolVLM](https://github.com/ggml-org/SmolVLM-500M-Instruct-GGUF) - Pour le modÃ¨le de vision
- [OpenAI](https://openai.com) - Pour les modÃ¨les cloud
- [ngxson/smolvlm-realtime-webcam](https://github.com/ngxson/smolvlm-realtime-webcam) - Pour l'inspiration initiale

---

**ðŸŽ‰ Amusez-vous bien avec votre dÃ©tection d'objets en temps rÃ©el !**
>>>>>>> b5bd18581e4a6b1f7a3909eec8f011c4fcfd2cb7
