#!/bin/bash

echo "ðŸš€ Installation de Llama.cpp pour la dÃ©tection d'objets par webcam"
echo "================================================================"

# VÃ©rifier si Homebrew est installÃ© (macOS)
if command -v brew &> /dev/null; then
    echo "âœ… Homebrew dÃ©tectÃ©"
else
    echo "âŒ Homebrew non trouvÃ©. Installation..."
    /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
fi

# Installer les dÃ©pendances
echo "ðŸ“¦ Installation des dÃ©pendances..."
brew install cmake ninja

# Cloner llama.cpp
echo "ðŸ“¥ Clonage de llama.cpp..."
if [ -d "llama.cpp" ]; then
    echo "âš ï¸  RÃ©pertoire llama.cpp existe dÃ©jÃ , mise Ã  jour..."
    cd llama.cpp
    git pull
else
    git clone https://github.com/ggml-org/llama.cpp.git
    cd llama.cpp
fi

# Compiler llama.cpp
echo "ðŸ”¨ Compilation de llama.cpp..."
mkdir -p build
cd build
cmake .. -DLLAMA_METAL=ON -DLLAMA_AVX=ON -DLLAMA_AVX2=ON
make -j$(nproc 2>/dev/null || sysctl -n hw.ncpu 2>/dev/null || echo 4)

echo "âœ… Installation terminÃ©e !"
echo ""
echo "ðŸ“‹ Pour dÃ©marrer le serveur Llama.cpp:"
echo "1. Allez dans le rÃ©pertoire: cd llama.cpp/build"
echo "2. Lancez le serveur: ./llama-server -hf ggml-org/SmolVLM-500M-Instruct-GGUF -ngl 99"
echo "3. Ouvrez l'interface web: http://localhost:8000"
echo ""
echo "ðŸ’¡ Options supplÃ©mentaires:"
echo "   - Pour utiliser le GPU: ajoutez -ngl 99"
echo "   - Pour changer le port: ajoutez -p 8080"
echo "   - Pour plus de mÃ©moire: ajoutez -c 4096"
